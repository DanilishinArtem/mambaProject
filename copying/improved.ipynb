{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef493fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Датасет для задачи копирования\n",
    "# ----------------------------\n",
    "class CopyDataset(Dataset):\n",
    "    def __init__(self, seq_len, vocab_size=100, num_samples=2000):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.vocab_size = vocab_size\n",
    "        # Генерируем случайные последовательности из [1, vocab_size)\n",
    "        self.data = torch.randint(1, vocab_size, (num_samples, seq_len))\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx]\n",
    "        return x, x  # таргет = тот же самый тензор\n",
    "\n",
    "class NoisyCopyDataset(CopyDataset):\n",
    "    def __getitem__(self, idx):\n",
    "        x, _ = super().__getitem__(idx)\n",
    "        noise = torch.zeros(10, dtype=x.dtype)  # токен 0 — шум\n",
    "        x_noisy = torch.cat([noise, x, noise], dim=0)\n",
    "        return x_noisy[:self.seq_len], x  # input truncated, target — исход\n",
    "# ----------------------------\n",
    "# 2) Модели\n",
    "# ----------------------------\n",
    "class LSTMCopy(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "    def forward(self, x):\n",
    "        emb = self.embed(x)  # (B, L, E)\n",
    "        out, _ = self.lstm(emb)  # (B, L, H)\n",
    "        logits = self.fc(out)    # (B, L, V)\n",
    "        return logits\n",
    "\n",
    "class TransformerCopy(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, nhead, num_layers):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(embed_dim, nhead, dim_feedforward=embed_dim*4)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "        self.fc = nn.Linear(embed_dim, vocab_size)\n",
    "    def forward(self, x):\n",
    "        emb = self.embed(x)            # (B, L, E)\n",
    "        emb = emb.permute(1,0,2)       # (L, B, E) для Transformer\n",
    "        out = self.encoder(emb)        # (L, B, E)\n",
    "        out = out.permute(1,0,2)       # (B, L, E)\n",
    "        logits = self.fc(out)          # (B, L, V)\n",
    "        return logits\n",
    "\n",
    "class MambaPlusPlus(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.linear_a = nn.Linear(embed_dim, hidden_dim)\n",
    "        self.linear_b = nn.Linear(embed_dim, hidden_dim)\n",
    "        self.linear_c = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "    def forward(self, x):\n",
    "        emb = self.embed(x)  # (B, L, E)\n",
    "        B, L, _ = emb.shape\n",
    "        h = torch.zeros(B, self.linear_c.in_features, device=x.device)\n",
    "        outputs = []\n",
    "        for t in range(L):\n",
    "            a_t = torch.sigmoid(self.linear_a(emb[:,t]))  # гейты\n",
    "            b_t = self.linear_b(emb[:,t])                  # входной сигнал\n",
    "            h = a_t * h + b_t                              # SSM-update\n",
    "            y_t = self.linear_c(h)                         # output projection\n",
    "            outputs.append(y_t.unsqueeze(1))\n",
    "        out = torch.cat(outputs, dim=1)                    # (B, L, H)\n",
    "        logits = self.fc(out)                              # (B, L, V)\n",
    "        return logits\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Функция обучения и оценки\n",
    "# ----------------------------\n",
    "def train_and_eval(model, train_loader, test_seq_lens, device, epochs=5):\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "    # Тренировка\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            logits = model(x)  # (B, L, V)\n",
    "            loss = criterion(logits.view(-1, logits.size(-1)), y.view(-1))\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss={total_loss/len(train_loader):.4f}\")\n",
    "    # Оценка точности на разных длинах\n",
    "    model.eval()\n",
    "    results = {}\n",
    "    with torch.no_grad():\n",
    "        for L in test_seq_lens:\n",
    "            # Формируем батч фиксированного размера\n",
    "            x_test = torch.randint(1, train_loader.dataset.vocab_size, (64, L), device=device)\n",
    "            y_test = x_test\n",
    "            logits = model(x_test)\n",
    "            preds = logits.argmax(dim=-1)\n",
    "            acc = (preds == y_test).float().mean().item()\n",
    "            results[L] = acc\n",
    "    return results\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Запуск всех моделей\n",
    "# ----------------------------\n",
    "def run_experiment():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    vocab_size = 100\n",
    "    embed_dim = 32\n",
    "    hidden_dim = 64\n",
    "\n",
    "    # Датасет длины 50\n",
    "    train_ds = NoisyCopyDataset(seq_len=50, vocab_size=vocab_size, num_samples=2000)\n",
    "    train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "\n",
    "    test_seq_lens = [50, 100, 200, 300, 500, 1000]\n",
    "\n",
    "    models = {\n",
    "        \"LSTM\": LSTMCopy(vocab_size, embed_dim, hidden_dim),\n",
    "        \"Transformer\": TransformerCopy(vocab_size, embed_dim, nhead=4, num_layers=2),\n",
    "        \"Mamba++\": MambaPlusPlus(vocab_size, embed_dim, hidden_dim)\n",
    "    }\n",
    "\n",
    "    all_results = {}\n",
    "    for name, model in models.items():\n",
    "        print(f\"\\n=== Training {name} ===\")\n",
    "        results = train_and_eval(model, train_loader, test_seq_lens, device, epochs=5)\n",
    "        all_results[name] = results\n",
    "\n",
    "    # Вывод таблицы\n",
    "    df = pd.DataFrame(all_results).T\n",
    "    df.columns = [f\"len={L}\" for L in df.columns]\n",
    "    print(\"\\n=== Final Accuracy ===\")\n",
    "    print(df)\n",
    "    df.to_csv(\"copy_task_results.csv\")\n",
    "    print(\"Saved results to copy_task_results.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22eeff84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# start = time.time()\n",
    "# for _ in range(100):\n",
    "#     _ = model(x_test)\n",
    "# print(\"Throughput:\", 100 * x_test.numel() / (time.time() - start), \"tokens/sec\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
