{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29bf6387",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adanilishin/miniconda3/lib/python3.12/site-packages/torch/nn/modules/transformer.py:382: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training LSTM ===\n",
      "Epoch 1/1, Loss=4.4528\n",
      "\n",
      "=== Training Transformer ===\n",
      "Epoch 1/1, Loss=3.6661\n",
      "\n",
      "=== Training Mamba++ ===\n",
      "Epoch 1/1, Loss=2.7015\n",
      "\n",
      "=== Final Accuracy ===\n",
      "               len=50   len=100   len=200   len=300   len=500  len=1000\n",
      "LSTM         0.567500  0.581719  0.577422  0.567708  0.567844  0.563953\n",
      "Transformer  0.844063  0.856562  0.862266  0.866094  0.866156  0.869266\n",
      "Mamba++      0.994062  0.993906  0.992969  0.992396  0.992188  0.992813\n",
      "Saved results to copy_task_results.csv\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Датасет для задачи копирования\n",
    "# ----------------------------\n",
    "class CopyDataset(Dataset):\n",
    "    def __init__(self, seq_len, vocab_size=100, num_samples=2000):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.vocab_size = vocab_size\n",
    "        # Генерируем случайные последовательности из [1, vocab_size)\n",
    "        self.data = torch.randint(1, vocab_size, (num_samples, seq_len))\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx]\n",
    "        return x, x  # таргет = тот же самый тензор\n",
    "\n",
    "class NoisyCopyDataset(CopyDataset):\n",
    "    def __getitem__(self, idx):\n",
    "        x, _ = super().__getitem__(idx)\n",
    "        noise = torch.zeros(1, dtype=x.dtype)  # токен 0 — шум\n",
    "        x_noisy = torch.cat([noise, x, noise], dim=0)\n",
    "        return x_noisy[:self.seq_len], x  # input truncated, target — исход\n",
    "# ----------------------------\n",
    "# 2) Модели\n",
    "# ----------------------------\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "    def forward(self, x):\n",
    "        emb = self.embed(x)  # (B, L, E)\n",
    "        out, _ = self.lstm(emb)  # (B, L, H)\n",
    "        logits = self.fc(out)    # (B, L, V)\n",
    "        return logits\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, nhead, num_layers):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(embed_dim, nhead, dim_feedforward=embed_dim*4)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "        self.fc = nn.Linear(embed_dim, vocab_size)\n",
    "    def forward(self, x):\n",
    "        emb = self.embed(x)            # (B, L, E)\n",
    "        emb = emb.permute(1,0,2)       # (L, B, E) для Transformer\n",
    "        out = self.encoder(emb)        # (L, B, E)\n",
    "        out = out.permute(1,0,2)       # (B, L, E)\n",
    "        logits = self.fc(out)          # (B, L, V)\n",
    "        return logits\n",
    "\n",
    "class MambaPlusPlus(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_heads):\n",
    "        super().__init__()\n",
    "        assert hidden_dim % num_heads == 0, \"hidden_dim must be divisible by num_heads\"\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = hidden_dim // num_heads\n",
    "        \n",
    "        # Parameter generators for each head\n",
    "        self.W_a = nn.ModuleList([nn.Linear(embed_dim, self.head_dim) for _ in range(num_heads)])\n",
    "        self.W_b = nn.ModuleList([nn.Linear(embed_dim, self.head_dim) for _ in range(num_heads)])\n",
    "        self.C   = nn.ModuleList([nn.Linear(self.head_dim, self.head_dim) for _ in range(num_heads)])\n",
    "        \n",
    "        # Projection and FFN\n",
    "        self.proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.norm = nn.LayerNorm(hidden_dim)\n",
    "        self.ffn1 = nn.Linear(hidden_dim, hidden_dim * 4)\n",
    "        self.ffn2 = nn.Linear(hidden_dim * 4, hidden_dim)\n",
    "        self.act = nn.GELU()\n",
    "        self.output_fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, L)\n",
    "        emb = self.embed(x)  # (B, L, E)\n",
    "        B, L, _ = emb.shape\n",
    "        # Initialize hidden states for each head\n",
    "        h = [torch.zeros(B, self.head_dim, device=x.device) for _ in range(self.num_heads)]\n",
    "        head_outputs = []\n",
    "        \n",
    "        # SSM update per time step\n",
    "        for t in range(L):\n",
    "            head_outs_t = []\n",
    "            for i in range(self.num_heads):\n",
    "                a_t = torch.tanh(self.W_a[i](emb[:, t]))  # gating\n",
    "                b_t = self.W_b[i](emb[:, t])             # input transform\n",
    "                h[i] = a_t * h[i] + b_t                   # SSM recurrence\n",
    "                head_out = self.C[i](h[i])                # output projection\n",
    "                head_outs_t.append(head_out)\n",
    "            # Concatenate heads\n",
    "            concat = torch.cat(head_outs_t, dim=-1)       # (B, hidden_dim)\n",
    "            head_outputs.append(concat.unsqueeze(1))\n",
    "        \n",
    "        z = torch.cat(head_outputs, dim=1)  # (B, L, hidden_dim)\n",
    "        # Residual + Norm\n",
    "        u = z + self.norm(z)\n",
    "        # Feed-Forward Network\n",
    "        ffn_out = self.act(self.ffn1(u))\n",
    "        o = self.ffn2(ffn_out)\n",
    "        h_out = u + o  # residual\n",
    "        # Final output\n",
    "        logits = self.output_fc(self.proj(h_out))\n",
    "        return logits\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Функция обучения и оценки\n",
    "# ----------------------------\n",
    "def train_and_eval(model, train_loader, test_seq_lens, device, epochs=5):\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "    # Тренировка\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            logits = model(x)  # (B, L, V)\n",
    "            loss = criterion(logits.view(-1, logits.size(-1)), y.view(-1))\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss={total_loss/len(train_loader):.4f}\")\n",
    "    # Оценка точности на разных длинах\n",
    "    model.eval()\n",
    "    results = {}\n",
    "    with torch.no_grad():\n",
    "        for L in test_seq_lens:\n",
    "            # Формируем батч фиксированного размера\n",
    "            x_test = torch.randint(1, train_loader.dataset.vocab_size, (64, L), device=device)\n",
    "            y_test = x_test\n",
    "            logits = model(x_test)\n",
    "            preds = logits.argmax(dim=-1)\n",
    "            acc = (preds == y_test).float().mean().item()\n",
    "            results[L] = acc\n",
    "    return results\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Запуск всех моделей\n",
    "# ----------------------------\n",
    "def run_experiment():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    vocab_size = 100\n",
    "    embed_dim = 32\n",
    "    hidden_dim = 64\n",
    "\n",
    "    # Датасет длины 50\n",
    "    train_ds = CopyDataset(seq_len=50, vocab_size=vocab_size, num_samples=2000)\n",
    "    # train_ds = NoisyCopyDataset(seq_len=50, vocab_size=vocab_size, num_samples=2000)\n",
    "    train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "\n",
    "    test_seq_lens = [50, 100, 200, 300, 500, 1000]\n",
    "\n",
    "    models = {\n",
    "        \"LSTM\": LSTM(vocab_size, embed_dim, hidden_dim),\n",
    "        \"Transformer\": Transformer(vocab_size, embed_dim, nhead=8, num_layers=8),\n",
    "        \"Mamba++\": MambaPlusPlus(vocab_size, embed_dim, hidden_dim, num_heads=2)\n",
    "    }\n",
    "\n",
    "    all_results = {}\n",
    "    for name, model in models.items():\n",
    "        print(f\"\\n=== Training {name} ===\")\n",
    "        results = train_and_eval(model, train_loader, test_seq_lens, device, epochs=1)\n",
    "        all_results[name] = results\n",
    "\n",
    "    # Вывод таблицы\n",
    "    df = pd.DataFrame(all_results).T\n",
    "    df.columns = [f\"len={L}\" for L in df.columns]\n",
    "    print(\"\\n=== Final Accuracy ===\")\n",
    "    print(df)\n",
    "    df.to_csv(\"copy_task_results.csv\")\n",
    "    print(\"Saved results to copy_task_results.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_experiment()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
